{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c76dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e4dda520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        # Save everything to analyse later\n",
    "        self.layers = layers\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        self.activations = []\n",
    "        self.z_list = []\n",
    "        \n",
    "        # Initialize parameters with random values\n",
    "        # Weight Shape = [l, l-1]\n",
    "        # We do layers[i+1] because layers also has input size in it\n",
    "        for i in range(len(layers)-1):\n",
    "            self.weights.append(np.random.random([layers[i+1], layers[i]]))\n",
    "            self.bias.append(np.random.random([layers[i+1], 1]))\n",
    "    \n",
    "    def forward_pass(self, a):\n",
    "        z_list = []\n",
    "        activations = [a] # activation also include inputs so we can use it later\n",
    "        for i in range(len(self.layers)-1):\n",
    "            z = np.matmul(self.weights[i], a) + self.bias[i] # z(l,1) = W(l,l-1)*a(l-1,1) + b(l,1)\n",
    "            a = self.sigmoid(z) # a(l,1)\n",
    "            z_list.append(z)\n",
    "            activations.append(a)\n",
    "        self.z_list = z_list\n",
    "        self.activations = activations\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, target, learning_rate):\n",
    "        \n",
    "        # First the output layer calculations\n",
    "        output = self.activations[-1]\n",
    "        _, del_loss_out = mlp.cross_entropy_loss(target, output)\n",
    "        del_l = del_loss_out * self.sigmoid_derivative(self.z_list[-1]) # del_l(1,1)\n",
    "        \n",
    "        # For other layers\n",
    "        for layer in range(len(self.layers)-1,0,-1):\n",
    "            \n",
    "            del_w = np.matmul(del_l, self.activations[layer-1].T) # del_w(l,l-1) = del_l(l,1)*a(l-1,1).T = del_l(l,1)*a(1,l-1)\n",
    "            del_b = del_l # del_b(l,1) = del_l(l,1)\n",
    "            self.weights[layer-1] -= learning_rate * del_w\n",
    "            self.bias[layer-1] -= learning_rate * del_b\n",
    "            \n",
    "            # del_l for previous layer, del_l(l-1, 1) = weights(l, l-1).T*del_l(l,1) ** derivative(l-1)\n",
    "            del_l = np.matmul(self.weights[layer-1].T, del_l) * self.sigmoid_derivative(self.z_list[layer-2])\n",
    "    \n",
    "    # Print functions\n",
    "    def get_weights(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            print(f'=============Layer {i+1} Weights================')\n",
    "            print(f'Weights: {self.weights[i]} {self.weights[i].shape}')\n",
    "            print(f'Bias: {self.bias[i]} {self.bias[i].shape}')\n",
    "    \n",
    "    def get_activations(self):\n",
    "        print(f'=============Input ================')\n",
    "        print(f'Weights: {self.activations[0]} {self.activations[0].shape}')\n",
    "        for i in range(1, len(self.activations)):\n",
    "            print(f'=============Layers {i} activations================')\n",
    "            print(f'Weights: {self.activations[i]} {self.activations[i].shape}')\n",
    "            \n",
    "    def get_z_values(self):\n",
    "        for i in range(len(self.z_list)):\n",
    "            print(f'=============Layers {i+1} z values================')\n",
    "            print(f'Weights: {self.z_list[i]} {self.z_list[i].shape}')\n",
    "    \n",
    "    # Utility functions\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    def cross_entropy_loss(self, target, output):\n",
    "        loss = -target*np.log(output) -(1-target)*np.log(1-output)\n",
    "        d_loss_out = (output - target)/(output - output**2)\n",
    "        return loss, d_loss_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1867f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP([2,3,1])\n",
    "inputs = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "targets = np.array([0,0,0,1])\n",
    "\n",
    "for _ in range(1000):\n",
    "    for inp, target in zip(inputs, targets):\n",
    "        inp = inp.reshape(-1,1)\n",
    "        activations = mlp.forward_pass(inp)\n",
    "        mlp.backward_pass(target, 0.1)\n",
    "\n",
    "# mlp.get_weights()\n",
    "# mlp.get_activations()\n",
    "# mlp.get_z_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cbaadfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input [0 0]: 0.00\n",
      "For input [1 0]: 0.01\n",
      "For input [0 1]: 0.01\n",
      "For input [1 1]: 0.98\n"
     ]
    }
   ],
   "source": [
    "for inp in inputs:\n",
    "    inp = inp.reshape(-1,1)\n",
    "    activations = mlp.forward_pass(inp)\n",
    "    activations = mlp.forward_pass(inp)\n",
    "    output = activations[-1]\n",
    "    print(f'For input {inp.reshape(1,-1)[0]}: {output[0][0]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635efca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
